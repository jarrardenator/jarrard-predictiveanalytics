{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note- This content is copyrighted material:  \n",
    "CC-BY Attribution 4.0 International Lynd Bacon & Associates, Ltd. DBA Loma Buena Associates  www.LBA.com  \n",
    "For information on permissible use, see https://creativecommons.org/licenses/by-sa/4.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you'll work with data collected over three years from persons at the San Francisco International Airport (SFO).\n",
    "\n",
    "Like many organizations, SFO regularly assesses who its customers are, and what they like and don't like.\n",
    "\n",
    "You can find out more about the SFO data at http://www.flysfo.com/media/customer-survey-data\n",
    "\n",
    "The data you'll be working with is stored in different file formats and coded in somewhat different ways year to year.  Some variables have the same names year to year, while others have names that vary over the years. It will become apparent to you as you review the \"data dictionaries\" provided.  You'll be producing some summary results from the data and creating and storing new data sets for subsequent analyses and research.  This exercise exemplifies tasks that are common when the objective is to whip customer data into shape for analysis.\n",
    "\n",
    "This exercise, like most of those in PRED 420, involves using the `Pandas` Python package for working with data.  You'll be using Python and `Pandas` to input data and to do common data manipulation tasks.   `Pandas` is a very extensive package that provides many methods for working with and summarizing data.  Online documentation is at http://pandas.pydata.org is current, but can be a little overwhelming at times.  The book by McKinney (the original author of `Pandas`) is accessible but is dated.  If you want a gentler introduction to `Pandas`, Try Harrison's \"Learning Pandas\" book.  It's an easy read.  For starters pay particular attention to the DataFrames sections.\n",
    "\n",
    "You'll find a lot about `Pandas` and Python in general online at sites like http://www.stackoverflow.com, http://www.github.com, and others.\n",
    "\n",
    "Post your questions, issues, and other things about this assignment to the GrEx1 Huddle on Canvas.  You're encouraged to help each other.  But be sure to turn in your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SFO survey data for each of the years 2014, 2015, and 2016 are in zip files. Each zip file contains a data file that's either an xls file or a csv, and a pdf document that is the _data dictionary_ for the file.  This document indicates what the codes mean in each data field of the data file it is for.  To understand what the data mean for any particular year, you need to use the information that's in the data dictionary for that year.\n",
    "\n",
    "By inspecting the files and their data dictionaries you'll note that the file contents are similar from year to year, but are not identical. For example, the numeric or alphabetic codes used to represent specific things that survey respondents said may differ. In one year, a 1 may mean a \"dirty fountain\" cleanliness comment, while in a different year the same comment may be coded as a 9.  So when working with the data from more than one year, attention to what the data codes are is necessary.\n",
    "\n",
    "In addition to the survey data there is a csv file named `select_resps_2017.csv` that contains the respondent IDs for respondents that are to be invited to participate in follow-up focus groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Python session or a Jupyter Notebook, read each data file into a `Pandas` DataFrame.  Use the `Pandas` input method appropriate for each file's type. You'll probably find the `Pandas` input methods to be the most convenient way to read the daa files. (If you are adventurous, read the xls files using the `openpyxl` package.) \n",
    "\n",
    "Then, do the next five parts.  Note that most of the following parts have more than one objective and/or deliverable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---Part 1---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1 **Create a single summary data set of ratings data using the data from all three years.**_ Your new data set should include all rating scale response data (e.g. \"Rating SFO\" responses, \"Cleanliness of SFO\" responses, safety-ness item responses, getting to the airport items, and so on; see below), unique respondent IDs, residence location (Q16LIVE in the 2016 data).  Create this data set in Python as a `Pandas` DataFrame. \n",
    "\n",
    "The response data are numeric data (although not necessarily metric) and should be treated as such. Make sure that the records in your DataFrame can be tied back to the original data sets by way of the unique respondent IDs or by other means. Include a variable indicating the year of data collection.  Each row in this DataFrame should correspond to one survey respondent.  The columns (or fields in the csv file you'll write, below) should be the ratings and other variables.\n",
    "\n",
    "Rating scale data are data that are captured by getting responses on a numeric scale that indicates some kind of magnitude, degree, or ordering.  For example, a cleanliness rating scale might have response categories ranging from 1 = poor to 5 = excellent.  Other numeric codes may have been used in various places in the data.  Note how the coding may differ from year to year.  You'll need to \"reconcile\" different codings in order to put the data from the different years together in a way that makes sense.  Be sure you treat missing values in a sensible manner.  See the *Note* about missing data, below.\n",
    "\n",
    "The data in the DataFrame you are creating is intended for use in multi-year trend analyses.  (You or your classmates may end up needing to use it later in the course.)\n",
    "\n",
    "(a) List the the variables from each year that you used to create this data set _using the original names they had in the data set_ they appeared in.\n",
    "\n",
    "(b) Document any variable name changes so that it's clear what the original variables are whose names you changed. (Otherwise, a user of your data wouldn't be able to know what these variables are, or how to use them.)\n",
    "\n",
    "(c) Describe your DataFrame in terms of its size, the variables in it, and how the data types of the variables.  How many missing values do you have on the ratings variables?\n",
    "\n",
    "(d) Write your new DataFrame to a csv file with an initial header record that includes the variable names.    Verify that you wrote this file without errors.\n",
    "\n",
    "Be sure to read the **NOTE** below about dealing with missing values and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A Little More on What Should Be in Your DataFrame for Part 1.__\n",
    "\n",
    "The ratings scale variables that you should include are in the following categories:\n",
    "\n",
    "* General assessments of aspects of the airport.  (In the 2016 data, these are Q7ART through Q7ALL)\n",
    "* Assessments of cleanliness (Q9BOARDING through Q9ALL in 2016)\n",
    "* Impression of safety (Q10SAFE in 2016)\n",
    "* Impression of PreCheck (Q12PRECHECKRATE in 2016)\n",
    "* Impression of ease of getting to the airport (Q13GETRATE in 2016)\n",
    "* Ease of finding way around the airport (Q14FIND in 2016)\n",
    "* Ease of getting through security (Q14PASSTHRU in 2016)\n",
    "\n",
    "Your DataFrame should include ratings variables for the above categories that were measured _in any of the three years_ of data collection.  For example, if in 2015 a rating of loadspeaker announcement understandability was collected, this rating variable should be included in the data set you create, even if the measure was not used in 2016 and 2014.  In other words, the ratings variables you should include should be the *union* of the sets of variables measured as ratings in any of the years, and not the intersection of the sets of rating variables measured.\n",
    "\n",
    "You'll also need to include for each respondent an unique respondent identifier, the year that the respondent was surveyed, and where the respondent lives relative to the airport (Q16LIVE in 2016).  You'll need the latter for __3__, below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---Part 2---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2__ __*Identify the top three (3) comments made in survey years 2015 and 2016.*__   Report the top three (3) most common comments based on their _relative frequency_. Include each of the comments' text that's indicated in the data dictionary documents.  For each of these comments, indicate its relative frequency. *Relative frequency* is the percentage or proportion of times that something happens. Be sure to give careful thought as to how to compute these relative frequencies. What are they percentages of? Document your method for computing them.\n",
    "\n",
    "The comments to consider for this part are Q8COM1-Q8COM3 in the 2015 data and Q8COM1-Q8COM5 in the 2016 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---Part 3---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3__  *__Using the data you created in Part 1, summarize the distribution of the SFO Airport \"as a whole\" rating data by respondent residence location category and report your results.__*  These are ratings about the airport, overall.  For example, in the 2016 data, the variable is called `Q7ALL`.  Make sure how you do this takes into account the nature of the data for this variable.  Think of this data as like a statistical sample.  How would you summarize it, statistically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---Part 4---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4__ __*Profile respondents for follow-up participation in qualitative research by creating a DataFrame that describes them.*__  Further investigation of some of the issues apparent in the survey data is going to be pursued by inviting selected respondents to participate in follow-up research.  The file `select_resps_FA2017.csv` identifies the respondents to be targeted. These respondents were surveyed in either 2015 or 2016.\n",
    "\n",
    "(a) Create a `Pandas` DataFrame of these respondents that includes data on their demographics and on the travel behaviors they reported when they were surveyed.  (See below.)  Include for each respondent the date and time he or she was interviewed.  \n",
    "\n",
    "(b) Save this DataFrame as a headered csv file. Verify that you saved it without errors.  Also, describe it in terms of its size and the variables in it. Be sure that you are clear on what what a demographic variable is, and what 'travel behavior' is. Be sure to preserve original variable names.\n",
    "\n",
    "Here are the kinds of variables you should include in your profile data set:\n",
    "\n",
    "* Respondent ID\n",
    "* Year surveyed\n",
    "* Destination geographic area\n",
    "* Size of destination market\n",
    "* Purpose(s) of travel  (Make sure that the meanings of the codes are consistent, e.g. that a code of \"5\" means the same thing year to year.)\n",
    "* Used parking?\n",
    "* Checked baggage?\n",
    "* Purchased from a store?\n",
    "* Purchased in a restaurant?\n",
    "* Used free WiFi?\n",
    "* Times Flown in last 12 mo.\n",
    "* First time flying out of SFO?\n",
    "* How Long Using SFO?\n",
    "* Residence Location?  Bay Area, or ...?  (Q16LIVE)\n",
    "* Age\n",
    "* Gender\n",
    "* Income\n",
    "* Fly 100K miles or more per year?\n",
    "* Language version of questionnaire\n",
    "* Have used the San Jose airport\n",
    "* Have used the Oakland airport\n",
    "\n",
    "Be sure to \"reconcile\" any differences in coding or variable names across the years.  \n",
    "\n",
    "(c) Tabulate the frequencies of the codes for Parking, Times Flown, Gender, How Long Using SFO.  This means for each of these variables create a frequency table showing how many times each coded value occurs, a table that explicitly includes a count of any missing values, if there are any. Why do you think it might be important to explicitly account for missing values when summarizing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----Part 5---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5__ __*Save each of the DataFrames you created above by \"pickling.\"*__  Pickling is a venerable Python technique for \"serializing\" Python objects.  Objects created in Python sessions doe not persist after a session ends by default. Verify that your pickling worked by unpickling each of your pickles.  (If you are feeling adventurous, also try saving your data sets in a `shelve` database.  A `shelve` database is like a permanent Python dictionary.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to save all of your data as you might need some of it later in the course.  It's usually a good idea to save your documented code, too, of course.  Save early and often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Do It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this entire assignment using just Python.  It's important to practice your Python skills for working with data.  That's what this course is about.  Resist the temptation to resort to Excel or other tools.\n",
    "\n",
    "For each of the above objectives, include in your assignment submission syntactically correct, commented Python code, along with any results that each part requests.  Do *not* first report all of your commented code, and then all requested results. Report any results for each part, part by part.\n",
    "\n",
    "Note that your Python code must be _syntactically correct_ in order to get credit for it.  \"Syntactically correct\" doesn't mean that it necessarily works to produce the required result.  What it means is that it is consistent with Python style guidelines, that indenting where necessary is correct, that lines of code are not truncated because of their length, that all parentheses and brackets are \"balanced,\" and so on. \"Correct\" means that a person could copy and paste it and it would work without producing syntax errors.  It also means that the comments included as sufficient to explain to another person what the code does. (Or is supposed to do.)\n",
    "\n",
    "Submit your work on Canvas in a pdf file, an html file that you have exported from a Jupyter Notebook, or a .doc or .docx file that is __no longer than seven (7) pages in length__, and in font size that is no smaller that 12 point. Any pages beyond seven will not be considered during grading.  Submissions that are not readable won't be graded. The above-mentioned file types are those that can be annotated when grading them on Canvas.  That's not the case for a Notebook file, unfortunately.\n",
    "\n",
    "Don't forget that a key attribute of \"good\" python code is that it's readable.\n",
    "\n",
    "Also, remember that assignments turned in late may be penalized.  See the course syllabus for the policy on late work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by reading all of the above, carefully.\n",
    "\n",
    "Next, *plan out* what you want to do for each of the above parts of this assignment before jumping headlong into wailing away at the data.  Outlining a plan of attack will help you organize your thinking about what needs to be done, which is useful even if you end up \"disrespecting\" your plan as you dig into the data.\n",
    "\n",
    "Many things are easier to do if they are broken down into small steps.   When you work with the survey data for the three years, think about first working with each year's data separately, and reviewing the dictionary for each one to make sure you understand what the variables are, and how they are coded.  If you're not clear on the variables' names and coding, you're unlikely to use the data correctly for the above assignment parts.\n",
    "\n",
    "There are some `Pandas` methods that are handy for doing what it is you need to do.  One often useful `Pandas` \"trick\" is that you can stack (concatenate, append) DataFrames and columns in them that have the same name will (usually) end up being combined appropriately.  Another trick is that you can rename multiple columns of a DataFrame by providing a dictionary of old name, new name pairs to the DataFrame's .rename() method.  A third trick is that you can use methods like .melt() to change a \"short and wide\" DataFrame to a \"tall and skinny\" one.  We'll cover some of these in a Sync Session, but don't delay on looking into them yourself. It'll be worth your time.\n",
    "\n",
    "A good thing to do when starting on Part 1 is to \"inventory\" the ratings variables in the data sets for the three years to see which ones are in common across the three years of data, which ones aren't in all years of data, and whether the names of any need to be changed before combining the data from the three years.  This is probably unavoidable variable \"bookkeeping\" if the assignment's objectives are going to be correctly met. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## NOTE: About Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with missing data is that it's not there, as a friend once said.  There are (at least) a couple of facets in dealing with it.  One is how to represent it symbolically, and other is how to best \"impute\" it, if that is want needs to be done.  Here we'll consider the first of these.  The latter can be a rather complex issue that is best left for another time.\n",
    "\n",
    "Missing values can be represented in data sets in various ways, and you'll see some of them in the data for this assignment.  Some ways are better than others, in the sense that they are unambiguous and are unlikely to result in data being misused. Pandas uses the numpy 'NaN' for missing numeric values.  If you want x to be a NaN, you'd assign it a value like x = np.nan, assuming that you had imported numpy as np.\n",
    "\n",
    "When you create your data sets for this assignment as `Pandas` DataFrames, try to use missing value codes that Python will recognize when doing calculations. For numeric missings, the numpy NaN is a \"natural\" choice.  What about missing character or string variable values? Practices regarding them vary.  When data containing them are read from a .csv file, they may be read as \"null\" (zero length) strings.  Or they may have been coded to represent a missing value, like None, or Null.  It's always a good idea to explore data to look for \"custom\" codings, anomalies, or unexpected oddities.\n",
    "\n",
    "For additional information about `Pandas` and missing values, see http://pandas.pydata.org/pandas-docs/stable/missing_data.html#missing-data\n",
    "\n",
    "If you want (more) gnarly details, check out https://docs.scipy.org/doc/numpy/neps/missing-data.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
